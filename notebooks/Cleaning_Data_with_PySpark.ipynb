{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cleaning Data with PySpark.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sjtalkar/DP-100AzureSupervisedUnsupervisedDatabricksAndSpark/blob/main/notebooks/Cleaning_Data_with_PySpark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Ijg5wUCTQYG"
      },
      "source": [
        "<p align=\"center\">\n",
        "<img src=\"https://github.com/datacamp/python-live-training-template/blob/master/assets/datacamp.svg?raw=True\" alt = \"DataCamp icon\" width=\"50%\">\n",
        "</p>\n",
        "<br><br>\n",
        "\n",
        "## **Cleaning Data with Pyspark**\n",
        "\n",
        "Welcome to this hands-on training where we will investigate cleaning a dataset using Python and Apache Spark! During this training, we will cover:\n",
        "\n",
        "* Efficiently loading data into a Spark DataFrame\n",
        "* Handling errant rows / columns from the dataset, including comments, missing data, combined or misinterpreted columns, etc.\n",
        "* Using Python UDFs to run advanced transformations on data\n",
        "\n",
        "\n",
        "## **The Dataset**\n",
        "\n",
        "The dataset used in this webinar is a set of CSV files named `netflix_titles_raw*.csv`. These contain information related to the movies and television shows available on Netflix. These are the *dirty* versions of the dataset - we will cover the individual problems as we work through the notebook.\n",
        "\n",
        "Given that this is a data cleaning webinar, let's look at our intended result.  The dataset will contain the follwing information:\n",
        "\n",
        "- `show_id`: A unique integer identifier for the show\n",
        "- `type`: The type of content, `Movie` or `TV Show`\n",
        "- `title`: The title of the content\n",
        "- `director`: The director (or directors)\n",
        "- `cast`: The cast\n",
        "- `country`: Country (or countries) where the content is available\n",
        "- `date_added`: Date added to Netflix\n",
        "- `release_year`: Year of content release\n",
        "- `rating`: Content rating\n",
        "- `duration`: The duration\n",
        "- `listed_in`: The genres the content is listed in\n",
        "- `description`: A description of the content\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLi6HZeKbiG4"
      },
      "source": [
        "## **Setting up a PySpark session**\n",
        "\n",
        "Before we can start processing our data, we need to configure a Pyspark session for Google Colab. Note that this is specific for using Spark and Python in Colab and likely is not required for other environments. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.3.1/spark-3.3.1-bin-hadoop3.tgz\n",
        "!tar xf spark-3.3.1-bin-hadoop3.tgz\n",
        "\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.3.1-bin-hadoop3\"\n",
        "\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n"
      ],
      "metadata": {
        "id": "VNwIoAjs50S5",
        "outputId": "c29cb508-c532-4281-baa5-24c565ee5a07",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "openjdk-8-jdk-headless is already the newest version (8u362-ga-0ubuntu1~20.04.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 23 not upgraded.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pyspark"
      ],
      "metadata": {
        "id": "7_qjwbGy3Jc_",
        "outputId": "5a6bc7b1-b995-4e4f-bd1b-e11562687e9d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyspark in /usr/local/lib/python3.9/dist-packages (3.3.2)\n",
            "Requirement already satisfied: py4j==0.10.9.5 in /usr/local/lib/python3.9/dist-packages (from pyspark) (0.10.9.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKvFmTnQbiG9"
      },
      "source": [
        "# Finally, setup our Spark session\n",
        "import findspark\n",
        "findspark.init()\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMYfcKeDY85K"
      },
      "source": [
        "## **Getting started**\n",
        "\n",
        "Before doing anything else, lets copy our data files locally. We'll be using `wget`, `ls`, `gunzip`, and `head`, which are normally shell commands. In the notebook environment, we can run any given shell command using the precursor `!`. The purpose of our shell commands are as follows:\n",
        "\n",
        "- `wget` - Is an HTTP client that will download our data files and save them locally in our notebook environment.\n",
        "- `ls` - Used to list the files in a directory.\n",
        "- `gunzip` - Our data files are compressed using the `gzip` compression format. `gunzip` allows us to decompress those files.\n",
        "- `head` - Much like the `.head()` command in Pandas, the shell command `head` defaults to printing out the first 10 lines of a file. You can specify more or fewer lines if desired. \n",
        "\n",
        "This is an example of some of these commands being executed in a traditional shell environment:\n",
        "\n",
        "![Shell Commands](https://github.com/datacamp/data-cleaning-with-pyspark-live-training/raw/master/assets/SparkLiveTraining-shellcommands.png)\n",
        "\n",
        "Let's run the follwing cell to pull the *dirty* files locally. We'll be writing the files to the `/tmp` directory in the notebook environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAfz_jiu0NjN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "baa6aec4-8a00-4dc6-e5a9-64629df4ae78"
      },
      "source": [
        "# Copy our dataset locally\n",
        "\n",
        "!wget -O /tmp/netflix_titles_dirty_01.csv.gz 'https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_01.csv.gz?raw=True'\n",
        "!wget -O /tmp/netflix_titles_dirty_02.csv.gz 'https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_02.csv.gz?raw=True'\n",
        "!wget -O /tmp/netflix_titles_dirty_03.csv.gz 'https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_03.csv.gz?raw=True'\n",
        "!wget -O /tmp/netflix_titles_dirty_04.csv.gz 'https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_04.csv.gz?raw=True'\n",
        "!wget -O /tmp/netflix_titles_dirty_05.csv.gz 'https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_05.csv.gz?raw=True'\n",
        "!wget -O /tmp/netflix_titles_dirty_06.csv.gz 'https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_06.csv.gz?raw=True'\n",
        "!wget -O /tmp/netflix_titles_dirty_07.csv.gz 'https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_07.csv.gz?raw=True'\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-03-23 19:21:18--  https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_01.csv.gz?raw=True\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/datacamp/data-cleaning-with-pyspark-live-training/raw/master/data/netflix_titles_dirty_01.csv.gz [following]\n",
            "--2023-03-23 19:21:19--  https://github.com/datacamp/data-cleaning-with-pyspark-live-training/raw/master/data/netflix_titles_dirty_01.csv.gz\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/datacamp/data-cleaning-with-pyspark-live-training/master/data/netflix_titles_dirty_01.csv.gz [following]\n",
            "--2023-03-23 19:21:19--  https://raw.githubusercontent.com/datacamp/data-cleaning-with-pyspark-live-training/master/data/netflix_titles_dirty_01.csv.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 157568 (154K) [application/octet-stream]\n",
            "Saving to: ‘/tmp/netflix_titles_dirty_01.csv.gz’\n",
            "\n",
            "/tmp/netflix_titles 100%[===================>] 153.88K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-03-23 19:21:19 (8.89 MB/s) - ‘/tmp/netflix_titles_dirty_01.csv.gz’ saved [157568/157568]\n",
            "\n",
            "--2023-03-23 19:21:19--  https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_02.csv.gz?raw=True\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/datacamp/data-cleaning-with-pyspark-live-training/raw/master/data/netflix_titles_dirty_02.csv.gz [following]\n",
            "--2023-03-23 19:21:19--  https://github.com/datacamp/data-cleaning-with-pyspark-live-training/raw/master/data/netflix_titles_dirty_02.csv.gz\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/datacamp/data-cleaning-with-pyspark-live-training/master/data/netflix_titles_dirty_02.csv.gz [following]\n",
            "--2023-03-23 19:21:19--  https://raw.githubusercontent.com/datacamp/data-cleaning-with-pyspark-live-training/master/data/netflix_titles_dirty_02.csv.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 156095 (152K) [application/octet-stream]\n",
            "Saving to: ‘/tmp/netflix_titles_dirty_02.csv.gz’\n",
            "\n",
            "/tmp/netflix_titles 100%[===================>] 152.44K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2023-03-23 19:21:19 (8.64 MB/s) - ‘/tmp/netflix_titles_dirty_02.csv.gz’ saved [156095/156095]\n",
            "\n",
            "--2023-03-23 19:21:19--  https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_03.csv.gz?raw=True\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/datacamp/data-cleaning-with-pyspark-live-training/raw/master/data/netflix_titles_dirty_03.csv.gz [following]\n",
            "--2023-03-23 19:21:20--  https://github.com/datacamp/data-cleaning-with-pyspark-live-training/raw/master/data/netflix_titles_dirty_03.csv.gz\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/datacamp/data-cleaning-with-pyspark-live-training/master/data/netflix_titles_dirty_03.csv.gz [following]\n",
            "--2023-03-23 19:21:20--  https://raw.githubusercontent.com/datacamp/data-cleaning-with-pyspark-live-training/master/data/netflix_titles_dirty_03.csv.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 156795 (153K) [application/octet-stream]\n",
            "Saving to: ‘/tmp/netflix_titles_dirty_03.csv.gz’\n",
            "\n",
            "/tmp/netflix_titles 100%[===================>] 153.12K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-03-23 19:21:20 (11.5 MB/s) - ‘/tmp/netflix_titles_dirty_03.csv.gz’ saved [156795/156795]\n",
            "\n",
            "--2023-03-23 19:21:20--  https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_04.csv.gz?raw=True\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/datacamp/data-cleaning-with-pyspark-live-training/raw/master/data/netflix_titles_dirty_04.csv.gz [following]\n",
            "--2023-03-23 19:21:20--  https://github.com/datacamp/data-cleaning-with-pyspark-live-training/raw/master/data/netflix_titles_dirty_04.csv.gz\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/datacamp/data-cleaning-with-pyspark-live-training/master/data/netflix_titles_dirty_04.csv.gz [following]\n",
            "--2023-03-23 19:21:20--  https://raw.githubusercontent.com/datacamp/data-cleaning-with-pyspark-live-training/master/data/netflix_titles_dirty_04.csv.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 157324 (154K) [application/octet-stream]\n",
            "Saving to: ‘/tmp/netflix_titles_dirty_04.csv.gz’\n",
            "\n",
            "/tmp/netflix_titles 100%[===================>] 153.64K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-03-23 19:21:21 (12.3 MB/s) - ‘/tmp/netflix_titles_dirty_04.csv.gz’ saved [157324/157324]\n",
            "\n",
            "--2023-03-23 19:21:21--  https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_05.csv.gz?raw=True\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/datacamp/data-cleaning-with-pyspark-live-training/raw/master/data/netflix_titles_dirty_05.csv.gz [following]\n",
            "--2023-03-23 19:21:21--  https://github.com/datacamp/data-cleaning-with-pyspark-live-training/raw/master/data/netflix_titles_dirty_05.csv.gz\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/datacamp/data-cleaning-with-pyspark-live-training/master/data/netflix_titles_dirty_05.csv.gz [following]\n",
            "--2023-03-23 19:21:21--  https://raw.githubusercontent.com/datacamp/data-cleaning-with-pyspark-live-training/master/data/netflix_titles_dirty_05.csv.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 155120 (151K) [application/octet-stream]\n",
            "Saving to: ‘/tmp/netflix_titles_dirty_05.csv.gz’\n",
            "\n",
            "/tmp/netflix_titles 100%[===================>] 151.48K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-03-23 19:21:21 (11.6 MB/s) - ‘/tmp/netflix_titles_dirty_05.csv.gz’ saved [155120/155120]\n",
            "\n",
            "--2023-03-23 19:21:21--  https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_06.csv.gz?raw=True\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/datacamp/data-cleaning-with-pyspark-live-training/raw/master/data/netflix_titles_dirty_06.csv.gz [following]\n",
            "--2023-03-23 19:21:22--  https://github.com/datacamp/data-cleaning-with-pyspark-live-training/raw/master/data/netflix_titles_dirty_06.csv.gz\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/datacamp/data-cleaning-with-pyspark-live-training/master/data/netflix_titles_dirty_06.csv.gz [following]\n",
            "--2023-03-23 19:21:22--  https://raw.githubusercontent.com/datacamp/data-cleaning-with-pyspark-live-training/master/data/netflix_titles_dirty_06.csv.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 157598 (154K) [application/octet-stream]\n",
            "Saving to: ‘/tmp/netflix_titles_dirty_06.csv.gz’\n",
            "\n",
            "/tmp/netflix_titles 100%[===================>] 153.90K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-03-23 19:21:22 (10.1 MB/s) - ‘/tmp/netflix_titles_dirty_06.csv.gz’ saved [157598/157598]\n",
            "\n",
            "--2023-03-23 19:21:22--  https://github.com/datacamp/data-cleaning-with-pyspark-live-training/blob/master/data/netflix_titles_dirty_07.csv.gz?raw=True\n",
            "Resolving github.com (github.com)... 192.30.255.112\n",
            "Connecting to github.com (github.com)|192.30.255.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/datacamp/data-cleaning-with-pyspark-live-training/raw/master/data/netflix_titles_dirty_07.csv.gz [following]\n",
            "--2023-03-23 19:21:22--  https://github.com/datacamp/data-cleaning-with-pyspark-live-training/raw/master/data/netflix_titles_dirty_07.csv.gz\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/datacamp/data-cleaning-with-pyspark-live-training/master/data/netflix_titles_dirty_07.csv.gz [following]\n",
            "--2023-03-23 19:21:22--  https://raw.githubusercontent.com/datacamp/data-cleaning-with-pyspark-live-training/master/data/netflix_titles_dirty_07.csv.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 39700 (39K) [application/octet-stream]\n",
            "Saving to: ‘/tmp/netflix_titles_dirty_07.csv.gz’\n",
            "\n",
            "/tmp/netflix_titles 100%[===================>]  38.77K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2023-03-23 19:21:22 (15.7 MB/s) - ‘/tmp/netflix_titles_dirty_07.csv.gz’ saved [39700/39700]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wo5BjsTS3nmy"
      },
      "source": [
        "**Now, let's verify that we have all 7 files we expect**\n",
        "\n",
        "Let's use the `ls` command to list files in the `/tmp` directory. Note that the `*` here is a wildcard, meaning match anything. Specifically, we're looking for `netflix_titles*`, which is match any file or directory that starts with `netflix_titles`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8QjJ77UpeS1N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e074026-f2ce-49df-91ff-0221015cc3db"
      },
      "source": [
        "!ls /tmp/netflix_titles*"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/tmp/netflix_titles_dirty_01.csv.gz  /tmp/netflix_titles_dirty_05.csv.gz\n",
            "/tmp/netflix_titles_dirty_02.csv.gz  /tmp/netflix_titles_dirty_06.csv.gz\n",
            "/tmp/netflix_titles_dirty_03.csv.gz  /tmp/netflix_titles_dirty_07.csv.gz\n",
            "/tmp/netflix_titles_dirty_04.csv.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NLnNIcR3z3G"
      },
      "source": [
        "**And then, we'll take a look at the first 20 rows of one of the files**\n",
        "\n",
        "As mentioned earlier, `gunzip` is a decompression tool. The `-c` means to print out the result rather than write it to a file. The `|`, or pipe symbol, is used to pass the output of one command as the input to another command. In this case, we're using `head -20` to print the first 20 lines of the decompressed data.\n",
        "\n",
        "Make sure to take a look at the contents of the data and notice that the separator used in this file is a tab character rather than a more traditional comma. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45574aapb4eX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7dd4b356-75f6-48d6-a82d-f141beb5a206"
      },
      "source": [
        "!gunzip -c /tmp/netflix_titles_dirty_03.csv.gz | head -20"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "80142103\tMovie\tBottom of the World\tRichard Sears\tJena Malone, Douglas Smith, Ted Levine, Tamara Duarte, Kelly Pendygraft, Mark Sivertsen, Jon McLaren\tCanada, United States\tMarch 31, 2017\t2017\tTV-MA\t84 min\tDramas, Independent Movies, Thrillers\tEn route to a fresh start in Los Angeles, young couple Alex and Scarlett stop over in a sleepy Southwestern town that loosens their grip on reality.\n",
            "80179907\tMovie\tBridget Christie: Stand Up for Her\t\tBridget Christie\tUnited Kingdom\tMarch 31, 2017\t2016\tTV-MA\t51 min\tStand-Up Comedy\tPerforming stand-up for a packed house in London's Hoxton Hall, comedian Bridget Christie dives into the politics of gender, sex and equality.\n",
            "80152842\tMovie\tFirstBorn\tNirpal Bhogal\tAntonia Thomas, Luke Norris, Thea Petrie, Eileen Davies, Jonathan Hyde\tUnited Kingdom\tMarch 31, 2017\t2016\tTV-MA\t90 min\tHorror Movies, International Movies\tA young couple fights supernatural foes in an attempt to save their daughter from the dark and mysterious forces that follow her every move.\n",
            "80049928\tTV Show\tFive Came Back\tLaurent Bouzereau\tSteven Spielberg, Guillermo del Toro, Lawrence Kasdan, Francis Ford Coppola, Paul Greengrass, Meryl Streep\tUnited States\tMarch 31, 2017\t2017\tTV-MA\t1 Season\tDocuseries\tFive acclaimed contemporary directors tell the story of five legendary Hollywood filmmakers who enlisted in the armed forces to document World War II.\n",
            "70296765\tMovie\tGLOW: The Story of the Gorgeous Ladies of Wrestling\tBrett Whitcomb\t\tUnited States\tMarch 31, 2017\t2012\tNR\t77 min\tDocumentaries, Sports Movies\tThis engaging documentary chronicles the 1980s phenomenon \"The Gorgeous Ladies of Wrestling,\" which elevated its female grapplers to star status.\n",
            "80131168\tMovie\tHiroshima: The Real History\tLucy van Beek\tJohn Sessions\tUnited Kingdom\tMarch 31, 2017\t2015\tTV-PG\t95 min\tDocumentaries\tThis detailed examination of the 1945 Hiroshima bombing includes the prelude to and aftermath of history's controversial first use of atomic weapons.\n",
            "80119190\tMovie\tKnow Your Enemy - Japan\tFrank Capra, Joris Ivens\tWalter Huston, Dana Andrews\tUnited States\tMarch 31, 2017\t1945\tTV-14\t63 min\tClassic Movies, Documentaries\tThough culturally insensitive by modern standards, this propaganda film examines the history of Japan from the 16th century through the 1930s.\n",
            "80119187\tMovie\tLet There Be Light\tJohn Huston\tWalter Huston\tUnited States\tMarch 31, 2017\t1946\tTV-PG\t58 min\tClassic Movies, Documentaries\tSome returning combat veterans suffer scars that are more psychological than physical. This film follows patients and staff during their treatment.\n",
            "80048568\tTV Show\tLondon Spy\tJakob Verbruggen\tBen Whishaw, Jim Broadbent, Edward Holcroft, Samantha Spiro, Charlotte Rampling, Lorraine Ashbourne, David Hayman, Clarke Peters, Adrian Lester, Harriet Walter\tUnited Kingdom\tMarch 31, 2017\t2015\tTV-MA\t1 Season\tBritish TV Shows, Crime TV Shows, International TV Shows\tWhen his reclusive-banker lover disappears, a hard-partying young British hedonist plunges into the dangerous world of espionage to find the truth.\n",
            "80178464\tTV Show\tMonkey Planet\t\tGeorge McGavin\tUnited Kingdom\tMarch 31, 2017\t2014\tTV-PG\t1 Season\tBritish TV Shows, Docuseries, Science & Nature TV\tJoin zoologist Dr. George McGavin for a fascinating, up-close look at the habits and behaviors of primates, our closest animal relatives.\n",
            "80119192\tMovie\tNazi Concentration Camps\tGeorge Stevens\t\tUnited States\tMarch 31, 2017\t1945\tTV-MA\t59 min\tClassic Movies, Documentaries\tShocking footage shows Nazi concentration camps after liberation, including Buchenwald and Ohrdruf, where Gen. Eisenhower ordered local Nazis to tour.\n",
            "70202593\tTV Show\tNorth & South\t\tDaniela Denby-Ashe, Richard Armitage, Tim Pigott-Smith, Sinéad Cusack, Lesley Manville, Brendan Coyle, Anna Maxwell Martin, Jo Joyner, Pauline Quirke, Kay Lyon\tUnited Kingdom\tMarch 31, 2017\t2004\tNR\t1 Season\tBritish TV Shows, Romantic TV Shows, TV Dramas\tWhen her father moves his family to an industrial mill town, the parson's daughter, Margaret Hale, struggles to adapt to her harsh new surroundings.\n",
            "60027945\tMovie\tPrelude to War\tFrank Capra\t\tUnited States\tMarch 31, 2017\t1942\tTV-PG\t52 min\tClassic Movies, Documentaries\tFrank Capra's documentary chronicles the rise of authoritarianism in Germany, Italy and Japan as America prepares for a defense of liberty.\n",
            "80119188\tMovie\tSan Pietro\tJohn Huston\t\tUnited States\tMarch 31, 2017\t1945\tTV-14\t32 min\tClassic Movies, Documentaries\tAfter the Allies invade Italy, the Liri Valley town of San Pietro becomes an example of the brutal effects of combat, both for soldiers and civilians.\n",
            "60027942\tMovie\tThe Battle of Midway\tJohn Ford\tHenry Fonda, Jane Darwell\tUnited States\tMarch 31, 2017\t1942\tTV-G\t18 min\tClassic Movies, Documentaries\tDirector John Ford captures combat footage of the Battle of Midway, an air and sea campaign that was a turning point in the war in the Pacific.\n",
            "80115857\tMovie\tThe Discovery\tCharlie McDowell\tJason Segel, Rooney Mara, Robert Redford, Riley Keough, Jesse Plemons, Ron Canada\tUnited States\tMarch 31, 2017\t2017\tTV-MA\t102 min\tDramas, Independent Movies, Romantic Movies\tA scientist whose proof of an afterlife caused a rash of suicides forges ahead with his research, while his disapproving son falls for a troubled woman.\n",
            "70281026\tTV Show\tThe Fear\tMichael Samuels\tPeter Mullan, Anastasia Hille, Harry Lloyd, Paul Nicholls, Demosthenes Chrysan, Dragos Bucur, Shaban Arifi, Julia Ragnarsson, Danny Sapani, Nigel Lindsay, Osy Ikhile, Sidney Kean, Lisa McAllister, Catherine Winter, Amarildo Kola, Julius Peter Wells\tUnited Kingdom\tMarch 31, 2017\t2012\tTV-MA\t1 Season\tBritish TV Shows, Crime TV Shows, TV Dramas\tRichie Beckett is an aging Brighton crime boss who's struggling with the onset of dementia, just as a rival gang is trying to take over his territory.\n",
            "70304244\tTV Show\tThe Great Train Robbery\t\tLuke Evans, Jim Broadbent, Paul Anderson, Martin Compston, Neil Maskell, Jack Roth, George Costigan, Robert Glenister, Nick Moran, Tim Pigott-Smith, James Fox, James Wilby, Jack Gordon, Nicholas Murchie, Del Synnott, Richard Hope, John Salthouse\tUnited Kingdom\tMarch 31, 2017\t2013\tNR\t1 Season\tBritish TV Shows, Crime TV Shows, International TV Shows\tThis two-part tale delivers the true story of the Great Train Robbery of 1963, the biggest one in England's history.\n",
            "80119194\tMovie\tThe Memphis Belle: A Story of a\n",
            "Flying Fortress\"\tWilliam Wyler\t\tUnited States\tMarch 31, 2017\t1944\tTV-PG\t40 min\tClassic Movies, Documentaries\tThis documentary centers on the crew of the B-17 Flying Fortress Memphis Belle as it prepares to execute a strategic bombing mission over Germany.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlIBIjOM39AS"
      },
      "source": [
        "## **Loading our initial DataFrame**\n",
        "\n",
        "Let's take a look at what Spark does with our data and see if it can properly parse the output. To do this, we'll first load the content into a DataFrame using the `spark.read.csv()` method. \n",
        "\n",
        "We'll pass in three arguments:\n",
        "\n",
        " - The path to the file(s)\n",
        " - An entry for `header=False`. Our files do not have a header row, so we must specify this or risk a data row being interpreted as a header.\n",
        " - The last argument we add is the `sep` option, which specifies the field separator. Often in CSV files this is a comma (`,`), but in our files it's a `\\t` or tab character.\n",
        "\n",
        "In our command, we'll be using the wildcard character, `*`, again to access all the files matching `/tmp/netflix_titles_dirty*.csv.gz` (ie, all the files we've downloaded thus far). Spark will handle associating all the files to the same dataframe. \n",
        "\n",
        "Depending on how familiar you are with Spark, you should note that this line does not actually read the contents of the files yet. This command is *lazy*, which means it's not executed until we request Spark to execute some type of analysis. This is a crucial point to understand how data processing behaves within Spark. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "id": "AIPD2_Zt9ZDr",
        "outputId": "7c273604-9cf6-4e4b-88ae-5f95f87ffc96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6238"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQ_z1ZFYb9AX"
      },
      "source": [
        "# Read csv file wiith spark.read_csv()\n",
        "df = spark.read.csv(\"/tmp/netflix_titles_dirty*.csv.gz\",header=False, sep=\"\\t\")"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUFy-nit4fmP"
      },
      "source": [
        "## **Initial analysis**\n",
        "\n",
        "Let's look at the first 150 rows using the `.show()` method on the DataFrame. We'll pass in:\n",
        "\n",
        "- The number of rows to display (`150`)\n",
        "- Set the `truncate` option to `False` so we can see all the DataFrame columns and content.\n",
        "\n",
        "*Note: `.show()` is a Spark action, meaning that any previous lazy commands will now be executed prior to the actual action. Spark can also optimize these commands for best performance as needed.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.count()"
      ],
      "metadata": {
        "id": "ePQOO1JJ8_xX",
        "outputId": "2f2e994a-91d8-491e-c7d7-08433a1bd46b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6238"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LjdckdCcCQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e0254487-c6f4-409f-9744-a3720fab5a03"
      },
      "source": [
        "# Show first 150 rows\n",
        "df.show(150)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-------+--------------------+--------------------+--------------------+--------------------+---------------+----+--------+--------+--------------------+--------------------+\n",
            "|                 _c0|    _c1|                 _c2|                 _c3|                 _c4|                 _c5|            _c6| _c7|     _c8|     _c9|                _c10|                _c11|\n",
            "+--------------------+-------+--------------------+--------------------+--------------------+--------------------+---------------+----+--------+--------+--------------------+--------------------+\n",
            "|            81002212|  Movie|            Bioscope|Gajendra Ahire, V...|Nina Kulkarni, Su...|               India|August 15, 2018|2015|   TV-14| 131 min|Dramas, Internati...|Inspired by class...|\n",
            "|            80176707|  Movie|  For Here or to Go?|   Rucha Humnabadkar|Ali Fazal, Melani...|United States, India|August 15, 2018|2015|   TV-MA| 105 min|Comedies, Dramas,...|A software engine...|\n",
            "|            80199381|  Movie|            Hostiles|        Scott Cooper|Christian Bale, R...|       United States|August 15, 2018|2017|       R| 134 min|              Dramas|After a long care...|\n",
            "|            80202176|  Movie|Hurricane Bianca:...|       Matt Kugelman|Roy Haylock, Rach...|       United States|August 15, 2018|2018|   TV-MA|  85 min|Comedies, Indepen...|When Texas teache...|\n",
            "|            81014293|TV Show| Nuestra Lucha Libre|                null|                null|                null|August 15, 2018|2018|   TV-MA|1 Season|Docuseries, Inter...|This documentary ...|\n",
            "|            81014811|TV Show|The Creative Indians|                null|                null|                null|August 15, 2018|2017|      NR|1 Season|Docuseries, Inter...|From chefs and co...|\n",
            "|            70123118|  Movie|What's Your Raashee?|  Ashutosh Gowariker|Priyanka Chopra, ...|               India|August 15, 2018|2009|   TV-PG| 203 min|Comedies, Interna...|To protect his fa...|\n",
            "|            80175913|TV Show|          20 Minutes|                null|Tuba Büyüküstün, ...|              Turkey|August 15, 2017|2013|   TV-MA|1 Season|Crime TV Shows, I...|When his wife is ...|\n",
            "|            80182274|  Movie|            Barbecue|      Matthew Salleh|                null|Australia, Armeni...|August 15, 2017|2017|   TV-MA| 101 min|Documentaries, In...|A blend of cultur...|\n",
            "|            80185166|  Movie|Brad Paisley's Co...|         Jay Chapman|Brad Paisley, Nat...|       United States|August 15, 2017|2017|   TV-MA|  63 min|     Stand-Up Comedy|Country music sta...|\n",
            "|            80068219|  Movie|Colin Quinn: Unco...|         Jay Chapman|         Colin Quinn|       United States|August 15, 2017|2015|   TV-MA|  56 min|     Stand-Up Comedy|New York comedian...|\n",
            "|            70209163|  Movie|                Goon|       Michael Dowse|Seann William Sco...|United States, Ca...|August 15, 2017|2011|       R|  92 min|Comedies, Sports ...|When he's seen di...|\n",
            "|            80170687|TV Show|   Million Yen Women|                null|Yojiro Noda, Rila...|               Japan|August 15, 2017|2017|   TV-MA|1 Season|Crime TV Shows, I...|Five beautiful bu...|\n",
            "|            80163191|  Movie|            My Ex-Ex|     Nathaniel Warsh|André Bharti, Kat...|              Canada|August 15, 2017|2015|   TV-MA|  91 min|Comedies, Romanti...|A recently dumped...|\n",
            "|            80184661|  Movie|    We're No Animals|   Alejandro Agresti|John Cusack, Paul...|United States, Ar...|August 15, 2017|2015|   TV-MA|  94 min|Comedies, Dramas,...|Unhappy with his ...|\n",
            "|            80114588|TV Show|  Behind Enemy Lines|                null|                null|       United States|August 15, 2016|2001|   TV-14|1 Season|British TV Shows,...|After dire setbac...|\n",
            "|            80078761|  Movie|            Being AP|       Anthony Wonke|          Tony McCoy|      United Kingdom|August 15, 2016|2015|   TV-14|  99 min|Documentaries, In...|With retirement s...|\n",
            "|            80115844|  Movie|         John & Jane|     Ashim Ahluwalia|                null|               India|August 15, 2016|2005|   TV-14|  79 min|Documentaries, In...|Truth and fiction...|\n",
            "|            80114111|  Movie|Louis C.K.: Live ...|          Louis C.K.|          Louis C.K.|       United States|August 15, 2016|2015|    null|  66 min|              Movies|The comic puts hi...|\n",
            "|            80108998|  Movie|Real Crime: Super...|                null|                null|      United Kingdom|August 15, 2016|2010|   TV-14|  46 min|       Documentaries|In 2000, U.K. det...|\n",
            "|            70308278|  Movie|        Mission Blue|Robert Nixon, Fis...|James Cameron, Mi...|United States, Be...|August 15, 2014|2014|   TV-Y7|  94 min|       Documentaries|This documentary ...|\n",
            "|            80223113|TV Show|          Happy Jail|                null|                null|                null|August 14, 2019|2019|   TV-MA|1 Season|Crime TV Shows, D...|The Philippine ja...|\n",
            "|            81147274|  Movie|               Uyare|        Manu Ashokan|Parvathy, Asif Al...|               India|August 14, 2019|2019|   TV-14| 119 min|Dramas, Internati...|An aspiring pilot...|\n",
            "|            80136450|TV Show|   Exclusive Edition|           James Lee|Berg Lee, Yise Lo...|                null|August 14, 2017|2008|   TV-14|1 Season|International TV ...|Young journalists...|\n",
            "|            80136453|TV Show|      Glowing Embers|                null|Henley Hii, Debbi...|                null|August 14, 2017|2010|   TV-14|1 Season|International TV ...|Amid Malaysia's c...|\n",
            "|            80163530|TV Show|             Persona|                null|Loo Aye Keng, Kyo...|                null|August 14, 2017|2015|   TV-PG|1 Season|Crime TV Shows, I...|A private investi...|\n",
            "|            80136455|TV Show|       The Iron Lady|                null|Yeo Yann Yann, Ae...|                null|August 14, 2017|2009|   TV-PG|1 Season|International TV ...|After business tr...|\n",
            "|            80039439|  Movie|        The Outcasts|     Peter Hutchings|Victoria Justice,...|       United States|August 14, 2017|2017|   PG-13|  94 min|            Comedies|After failing to ...|\n",
            "|            80044804|  Movie|Demetri Martin: L...|           Jay Karas|      Demetri Martin|       United States|August 14, 2015|2015|   TV-MA|  62 min|     Stand-Up Comedy|Demetri Martin br...|\n",
            "|            81147278|  Movie|               90 ML|         Anita Udeep|Oviya, Masoom Sha...|               India|August 13, 2019|2019|   TV-14| 123 min|Comedies, Interna...|Four friends shak...|\n",
            "|            81146011|  Movie|Calum von Moger: ...|          Vlad Yudin|     Calum Von Moger|       United States|August 13, 2019|2019|   TV-MA|  93 min|Documentaries, Sp...|Bodybuilder and t...|\n",
            "|            81123826|  Movie|   Hasta los dientes|Alberto Arnaut Es...|                null|              Mexico|August 13, 2019|2018|   TV-MA| 109 min|Documentaries, In...|This documentary ...|\n",
            "|            81002933|TV Show|Tiffany Haddish P...|                null|Tracey Ashley, Fl...|       United States|August 13, 2019|2019|   TV-MA|1 Season|Stand-Up Comedy &...|Hand-picked by Ti...|\n",
            "|            81135868|  Movie|           Woodstock|       Barak Goodman|                null|       United States|August 13, 2019|2019|   TV-MA|  97 min|Documentaries, Mu...|For the 50th anni...|\n",
            "|            80162142|  Movie|        Hot Property|          Max McGill|MyAnna Buring, To...|      United Kingdom|August 13, 2017|2016|   TV-MA|  82 min|Comedies, Indepen...|When a resourcefu...|\n",
            "|            80175483|  Movie|Mission Control: ...|      David Fairhead|                null|United States, Un...|August 13, 2017|2017|   TV-14| 100 min|       Documentaries|Combining archive...|\n",
            "|            80100772|  Movie|          13 Cameras|      Victor Zarcoff|PJ McCabe, Briann...|       United States|August 13, 2016|2015|      NR|  90 min|Horror Movies, In...|Young parents-to-...|\n",
            "|            81145135|TV Show| DC Super Hero Girls|                null|Grey Griffin, Tar...|       United States|August 12, 2019|2019|TV-Y7-FV|1 Season|            Kids' TV|As Metropolis Hig...|\n",
            "|            80188580|  Movie|Chocolate City: V...|Jean-Claude La Marre|Robert Ri'chard, ...|       United States|August 12, 2017|2017|   TV-MA|  90 min|              Dramas|The exotic dancer...|\n",
            "|            80106737|  Movie|       Los Herederos|Jorge Hernandez A...|Máximo Hollander,...|              Mexico|August 12, 2017|2015|   TV-MA|  71 min|Dramas, Internati...|Spurred by boredo...|\n",
            "|            80142058|  Movie|               Naked|      Michael Tiddes|Marlon Wayans, Re...|       United States|August 11, 2017|2017|   TV-14|  97 min|Comedies, Romanti...|Rob's madly in lo...|\n",
            "|            70044256|  Movie|Jim Gaffigan: Bey...|       Michael Drumm|        Jim Gaffigan|       United States|August 11, 2016|2005|   TV-14|  72 min|     Stand-Up Comedy|Comedian Jim Gaff...|\n",
            "|            70113636|  Movie|Jim Gaffigan: Kin...|         Troy Miller|        Jim Gaffigan|       United States|August 11, 2016|2009|   TV-PG|  71 min|     Stand-Up Comedy|Jim Gaffigan offe...|\n",
            "|            70237054|  Movie|Jim Gaffigan: Mr....|           Jay Karas|        Jim Gaffigan|       United States|August 11, 2016|2012|   TV-14|  77 min|     Stand-Up Comedy|Funnyman Jim Gaff...|\n",
            "|            70301469|  Movie|Jim Gaffigan: Obs...|         Jay Chapman|        Jim Gaffigan|       United States|August 11, 2016|2014|   TV-14|  70 min|     Stand-Up Comedy|Comic Jim Gaffiga...|\n",
            "|            80165437|TV Show|72 Dangerous Anim...|                null|        Bob Brisbane|           Australia|August 10, 2018|2018|   TV-14|1 Season|Docuseries, Inter...|From fangs to cla...|\n",
            "|            80188953|TV Show|           Afflicted|                null|                null|       United States|August 10, 2018|2018|   TV-MA|1 Season|Docuseries, Reali...|Baffling symptoms...|\n",
            "|            80211884|TV Show|All About the Was...|                null|Joseph Simmons, J...|       United States|August 10, 2018|2018|   TV-PG|1 Season|         TV Comedies|Hip-hop icon MC J...|\n",
            "|            81005454|  Movie|   Beyond the Clouds|        Majid Majidi|Ishaan Khattar, M...|         India, Iran|August 10, 2018|2018|   TV-14| 120 min|Dramas, Independe...|A youthful drug d...|\n",
            "|            80995575|  Movie|    The Birth Reborn|     Eduardo Chauvet|                null|              Brazil|August 10, 2018|2013|   TV-14|  91 min|Documentaries, In...|Mothers and medic...|\n",
            "|            80223371|  Movie|The Guernsey Lite...|         Mike Newell|Lily James, Michi...|United Kingdom, F...|August 10, 2018|2018|   TV-14| 124 min|Dramas, Romantic ...|A London writer b...|\n",
            "|            80175147|  Movie|         The Package|      Jake Szymanski|Daniel Doheny, Ge...|       United States|August 10, 2018|2018|   TV-MA|  95 min|            Comedies|After their buddy...|\n",
            "|            80239831|  Movie|                Zion|          Floyd Russ|          Zion Clark|       United States|August 10, 2018|2018|   TV-PG|  12 min|Documentaries, Sp...|Born without legs...|\n",
            "|            80105356|  Movie|   Alive and Kicking|       Susan Glatzer|                null|Sweden, United St...|August 10, 2017|2016|   TV-14|  89 min|       Documentaries|Take an inside lo...|\n",
            "|            70296443|  Movie|             Demonic|          Will Canon|Maria Bello, Fran...|       United States|August 10, 2017|2015|       R|  83 min|Horror Movies, In...|When amateur ghos...|\n",
            "|            80168033|  Movie|SHOT! The Psycho-...|         Barney Clay|           Mick Rock|United Kingdom, U...|August 10, 2017|null|   TV-MA|  98 min|Documentaries, Mu...|Aptly named iconi...|\n",
            "|            80160346|  Movie|           The Saint|     Ernie Barbarash|Adam Rayner, Eliz...|       United States|August 10, 2017|2017|   TV-14|  92 min|  Action & Adventure|Master thief Simo...|\n",
            "|            70121522|  Movie|            3 Idiots|     Rajkumar Hirani|Aamir Khan, Karee...|               India| August 1, 2019|2009|   PG-13| 164 min|Comedies, Dramas,...|While attending o...|\n",
            "|            81065784|  Movie|    Are We Done Yet?|  Khaled El Halafawy|Ahmed Eid, Ayten ...|               Egypt| August 1, 2019|2018|   TV-14|  87 min|Comedies, Dramas,...|When authorities ...|\n",
            "|            80124099|  Movie|   Boyka: Undisputed|     Todor Chapkanov|Scott Adkins, Teo...|Bulgaria, United ...| August 1, 2019|2016|       R|  90 min|  Action & Adventure|The fourth film i...|\n",
            "|            70119812|  Movie| Clash of the Titans|     Louis Leterrier|Sam Worthington, ...|United States, Un...| August 1, 2019|2010|   PG-13| 106 min|Action & Adventur...|If he is to save ...|\n",
            "|            60032685|  Movie|      Darna Mana Hai|        Prawal Raman|Aftab Shivdasani,...|               India| August 1, 2019|2003|   TV-MA| 116 min|Horror Movies, In...|Stranded in a jun...|\n",
            "|            70006816|  Movie|       Ek Hasina Thi|     Sriram Raghavan|Saif Ali Khan, Ur...|               India| August 1, 2019|2004|   TV-MA| 134 min|Dramas, Internati...|Imprisoned for un...|\n",
            "|            70236587|  Movie|  Ferrari Ki Sawaari|     Rajesh Mapuskar|Sharman Joshi, Bo...|               India| August 1, 2019|2012|   TV-PG| 133 min|Comedies, Interna...|A father winds up...|\n",
            "|              520489|  Movie|Four Weddings and...|         Mike Newell|Hugh Grant, Andie...|      United Kingdom| August 1, 2019|1994|       R| 118 min|Classic Movies, C...|A commitment-phob...|\n",
            "|            70266677|  Movie|               Horns|       Alexandre Aja|Daniel Radcliffe,...|United States, Ca...| August 1, 2019|2013|       R| 120 min|Dramas, Horror Mo...|Accused of murder...|\n",
            "|            60010514|  Movie|        Jackie Brown|   Quentin Tarantino|Pam Grier, Samuel...|       United States| August 1, 2019|1997|       R| 154 min|   Dramas, Thrillers|When an aging fli...|\n",
            "|            70301367|  Movie|   Jupiter Ascending|Lana Wachowski, L...|Mila Kunis, Chann...|United States, Au...| August 1, 2019|2015|   PG-13| 128 min|Action & Adventur...|A young impoveris...|\n",
            "|            81139317|TV Show|              Khaani|                null|Feroze Khan, Sana...|            Pakistan| August 1, 2019|2017|   TV-14|1 Season|Crime TV Shows, I...|After a rich poli...|\n",
            "|            80014727|  Movie|        Last Knights|      Kazuaki Kiriya|Clive Owen, Morga...|United Kingdom, S...| August 1, 2019|2015|       R| 115 min|  Action & Adventure|A nobleman who va...|\n",
            "|            81050688|  Movie|                Manu|  Phanindra Narsetti|Raja Goutham, Cha...|               India| August 1, 2019|2018|   TV-MA| 177 min|Dramas, Independe...|The relationship ...|\n",
            "|            80081505|  Movie|                MI-5|      Bharat Nalluri|Peter Firth, Kit ...|      United Kingdom| August 1, 2019|2015|       R| 105 min|Action & Adventur...|When a terrorist ...|\n",
            "|            60022700|  Movie|          Panic Room|       David Fincher|Jodie Foster, For...|       United States| August 1, 2019|2002|       R| 112 min|           Thrillers|A woman and her d...|\n",
            "|            81155880|TV Show|    Regiment Diaries|                null|                null|               India| August 1, 2019|2018|   TV-14|1 Season|Docuseries, Inter...|Historical footag...|\n",
            "|              915927|  Movie|               Rocky|    John G. Avildsen|Sylvester Stallon...|       United States| August 1, 2019|1976|      PG| 120 min|Action & Adventur...|Sylvester Stallon...|\n",
            "|              916043|  Movie|            Rocky II|  Sylvester Stallone|Sylvester Stallon...|       United States| August 1, 2019|1979|      PG| 119 min|Dramas, Sports Mo...|Featuring a rousi...|\n",
            "|            60010836|  Movie|           Rocky III|  Sylvester Stallone|Sylvester Stallon...|       United States| August 1, 2019|1982|      PG| 100 min|Dramas, Sports Mo...|After taking a po...|\n",
            "|              916061|  Movie|            Rocky IV|  Sylvester Stallone|Sylvester Stallon...|       United States| August 1, 2019|1985|      PG|  92 min|Dramas, Sports Mo...|Rocky Balboa take...|\n",
            "|            60025674|  Movie|             Rocky V|    John G. Avildsen|Sylvester Stallon...|       United States| August 1, 2019|1990|   PG-13| 104 min|Dramas, Sports Mo...|Inspired by the m...|\n",
            "|            70039168|  Movie|        Rumor Has It|          Rob Reiner|Jennifer Aniston,...|United States, Ge...| August 1, 2019|2005|   PG-13|  97 min|Comedies, Romanti...|When Sarah travel...|\n",
            "|            81103092|  Movie| Running Out Of Time|        Chris Stokes|Tasha Smith, RonR...|       United States| August 1, 2019|2018|   TV-14|  88 min|           Thrillers|Held hostage by m...|\n",
            "|            80242065|TV Show|Sleepless Society...|                null|Chermarn Boonyasa...|            Thailand| August 1, 2019|2019|   TV-MA|1 Season|International TV ...|Following the dea...|\n",
            "|            60031278|  Movie|Something's Gotta...|        Nancy Meyers|Jack Nicholson, D...|       United States| August 1, 2019|2003|   PG-13| 128 min|Comedies, Romanti...|Still sexy at 60,...|\n",
            "|            60001363|  Movie|       Space Cowboys|      Clint Eastwood|Clint Eastwood, T...|       United States| August 1, 2019|2000|   PG-13| 130 min|Action & Adventur...|A retired enginee...|\n",
            "|            81103517|TV Show|     The Chefs' Line|                null|Dan Hong, Mark Ol...|           Australia| August 1, 2019|2017|   TV-14|1 Season|International TV ...|Home cooks face o...|\n",
            "|            60026477|  Movie|To Wong Foo, Than...|       Beeban Kidron|Patrick Swayze, W...|       United States| August 1, 2019|1995|   PG-13| 108 min|Comedies, Cult Mo...|Three New York dr...|\n",
            "|            80094096|  Movie|               Wazir|       Bejoy Nambiar|Amitabh Bachchan,...|               India| August 1, 2019|2016|   TV-MA|  98 min|Dramas, Internati...|Helping an ampute...|\n",
            "|            81085824|  Movie|       White Chamber|        Paul Raschid|Shauna MacDonald,...|      United Kingdom| August 1, 2019|2018|   TV-MA|  89 min|Sci-Fi & Fantasy,...|When a civil war ...|\n",
            "|             1171655|  Movie|Bean: The Ultimat...|           Mel Smith|Rowan Atkinson, P...|United Kingdom, U...| August 1, 2018|1997|   PG-13|  89 min|Comedies, Interna...|Disaster-prone Mr...|\n",
            "|            80226279|TV Show|  Becoming Champions|                null|                null|              Mexico| August 1, 2018|2018|    TV-G|1 Season|Docuseries, Inter...|This series looks...|\n",
            "|            80993489|  Movie|      Being Napoleon|Jesse Handsher, O...|Mark Schneider, F...|       United States| August 1, 2018|2018|   TV-14|  88 min|       Documentaries|On the 200th anni...|\n",
            "|            70033643|  Movie|       Casino Tycoon|           Wong Jing|           Wong Jing|           Hong Kong| August 1, 2018|1992|   TV-MA| 125 min|Action & Adventur...|When Japanese tro...|\n",
            "|            70233317|  Movie|   Chernobyl Diaries|      Bradley Parker|Ingrid Bolsø Berd...|       United States| August 1, 2018|2012|       R|  88 min|Horror Movies, Th...|A group of kids t...|\n",
            "|            80997858|  Movie|Doubles Cause Tro...|           Wong Jing|Carol 'Do Do' Che...|           Hong Kong| August 1, 2018|1989|   TV-14|  94 min|Action & Adventur...|When the tenant i...|\n",
            "|            80238607|  Movie|       Flock of Four|      Gregory Caruso|Braeden Lemasters...|       United States| August 1, 2018|2018|   TV-MA|  83 min|Dramas, Independe...|One night in 1959...|\n",
            "|            80039485|  Movie|                Hero|          Corey Yuen|Takeshi Kaneshiro...|    Hong Kong, China| August 1, 2018|1997|   TV-MA|  89 min|Action & Adventur...|A pugilist from S...|\n",
            "|            80998560|  Movie|      History of Joy|    Vishnu Govindhan|Vishnu Vinay, Vin...|               India| August 1, 2018|2017|   TV-14| 118 min|Dramas, Internati...|The life of a hig...|\n",
            "|            70043379|  Movie|           Initial D|Andrew Lau Wai-Ke...|Jay Chou, Anne Su...|    China, Hong Kong| August 1, 2018|2005|   TV-14| 109 min|Action & Adventur...|By day, an 18-yea...|\n",
            "|            80232892|  Movie|           Invisible|     Pablo Giorgelli|Mora Arenillas, M...|   Argentina, France| August 1, 2018|2017|   TV-MA|  87 min|Dramas, Independe...|After learning sh...|\n",
            "|            80998562|  Movie|            Kaaliyan|        Jijo Pancode|Tini Tom, Meghana...|                null| August 1, 2018|2017|   TV-14| 105 min|Dramas, Internati...|An aggressive, wa...|\n",
            "|            70124268|  Movie|       Kung Fu Magoo|    Andrés Couturier|Dylan Sprouse, Co...|Mexico, United St...| August 1, 2018|2010|   TV-Y7|  79 min|Children & Family...|Shortsighted curm...|\n",
            "|            80998563|  Movie|           Kuppivala|       Suresh Pillai|Nandu, Kochu Prem...|                null| August 1, 2018|2017|   TV-14| 133 min|Dramas, Internati...|A young woman tal...|\n",
            "|            80998564|  Movie|              Lechmi|    B.N. Shajeer Sha|Parvathy Ratheesh...|               India| August 1, 2018|2017|   TV-14| 147 min|Comedies, Horror ...|The spirit of a m...|\n",
            "|            80997864|  Movie|Little Dragon Maiden|            Hua Shan|Leslie Cheung, Ji...|           Hong Kong| August 1, 2018|1983|   TV-14|  92 min|Action & Adventur...|Seeking to improv...|\n",
            "|            80035685|TV Show|Los tiempos de Pa...|   Alessandro Angulo|                null|            Colombia| August 1, 2018|2012|   TV-MA|1 Season|Crime TV Shows, D...|Featuring never-b...|\n",
            "|            70142792|  Movie|      Love In A Puff|      Pang Ho-cheung|Miriam Chin Wah Y...|           Hong Kong| August 1, 2018|2010|   TV-MA| 103 min|Comedies, Dramas,...|When the Hong Kon...|\n",
            "|            81002214|  Movie|          Mahabharat|          Amaan Khan|Amitabh Bachchan,...|               India| August 1, 2018|2013|   TV-14| 119 min|Action & Adventur...|Two young brother...|\n",
            "|            80993656|  Movie|               Mater|    Pablo D'Alo Abba|Lautaro Perotti, ...|           Argentina| August 1, 2018|2017|   TV-MA|  80 min|Dramas, Independe...|A single man in h...|\n",
            "|            80998568|  Movie|               Melle|     Binu Ulahhannan|Amith Chakalakkal...|               India| August 1, 2018|2017|   TV-PG| 110 min|Dramas, Internati...|After recovering ...|\n",
            "|            80998565|  Movie|Minnaminugu the F...|         Anil Thomas|Surabhi Lakshmi, ...|               India| August 1, 2018|2017|   TV-PG| 128 min|Dramas, Internati...|A nameless widow ...|\n",
            "|            80998566|  Movie|Mythily Veendum V...|       Sabu Varghese|Udhay, Kiran Raj,...|               India| August 1, 2018|2017|   TV-14|  88 min|Horror Movies, In...|After losing her ...|\n",
            "|            81001379|  Movie|Once In A Lifetim...|          Matt Askem|    Moby, Alex Cohen|                null| August 1, 2018|2018|   TV-PG|  86 min|Documentaries, Mu...|Through performan...|\n",
            "|            81001381|  Movie|Once In A Lifetim...|  Charlie Lightening|        Nile Rodgers|                null| August 1, 2018|2018|   TV-PG|  70 min|Documentaries, Mu...|The career of rhy...|\n",
            "|            81001382|  Movie|Once In A Lifetim...|  Charlie Lightening|      Noel Gallagher|                null| August 1, 2018|2018|   TV-MA|  66 min|Documentaries, Mu...|Between scenes fr...|\n",
            "|            81001383|  Movie|Once In A Lifetim...|          Matt Askem|                null|                null| August 1, 2018|2018|   TV-PG|  72 min|Documentaries, Mu...|One of the most s...|\n",
            "|            80998567|  Movie|Oru Vishsheshapet...|     Kiran Narayanan|Nedumudi Venu, V....|                null| August 1, 2018|2017|   TV-14| 122 min|Comedies, Dramas,...|When the village'...|\n",
            "|            81002215|  Movie|         P Se PM Tak|         Kundan Shah|Meenakshi Dixit, ...|               India| August 1, 2018|2015|   TV-14| 117 min|Comedies, Indepen...|A prostitute who ...|\n",
            "|            70105338|  Movie|R.L. Stine's Most...|     Richard Correll|Madison Pettis, S...|       United States| August 1, 2018|2008|      PG|  98 min|Children & Family...|After discovering...|\n",
            "|            81014824|  Movie|            Rangreza|      Amir Mohiuddin|Urwa Hocane, Bila...|            Pakistan| August 1, 2018|2017|   TV-14| 126 min|Dramas, Internati...|A high-flying roc...|\n",
            "|            81013200|  Movie| Sudani from Nigeria|            Zakariya|Soubin Shahir, Sa...|               India| August 1, 2018|2018|   TV-PG| 115 min|Comedies, Dramas,...|When a soccer clu...|\n",
            "|            80230018|TV Show|            Switched|                null|Daiki Shigeoka, T...|               Japan| August 1, 2018|2018|   TV-MA|1 Season|International TV ...|High schooler Ayu...|\n",
            "|            80242916|TV Show|The Could’ve-Gone...|                null|Jiro Sato, Mai Sh...|               Japan| August 1, 2018|2018|   TV-14|1 Season|International TV ...|People bring thei...|\n",
            "|            80041339|  Movie|      The Diabolical|    Alistair Legrand|Ali Larter, Arjun...|       United States| August 1, 2018|2015|   TV-MA|  86 min|Horror Movies, Sc...|Terrorized by une...|\n",
            "|            70105371|  Movie|      The Informant!|   Steven Soderbergh|Matt Damon, Scott...|       United States| August 1, 2018|2009|       R| 108 min|    Comedies, Dramas|While gathering e...|\n",
            "|            60022962|  Movie|  The Parole Officer|         John Duigan|Steve Coogan, Emm...|      United Kingdom| August 1, 2018|2001|       R|  94 min|Action & Adventur...|After witnessing ...|\n",
            "|            81002213|  Movie|Tikli and Laxmi Bomb|    Aditya Kripalani|Vibhawari Deshpan...|               India| August 1, 2018|2017|   TV-MA| 151 min|Dramas, Internati...|Two frustrated se...|\n",
            "|            80190147|  Movie|A Sort of Homecoming|        Maria Burton|Laura Marano, Par...|       United States| August 1, 2017|2015|   TV-PG|  89 min|Dramas, Independe...|Ace news producer...|\n",
            "|            80190149|  Movie|    All Hallows' Eve|      Charlie Vaughn|Lexi Giovagnoli, ...|       United States| August 1, 2017|2016|    TV-G|  92 min|Children & Family...|Instead of summon...|\n",
            "|            80195886|  Movie|   Anarkali of Aarah|         Avinash Das|Swara Bhaskar, Sa...|               India| August 1, 2017|2017|   TV-MA| 111 min|Dramas, Independe...|After being sexua...|\n",
            "|            80190148|  Movie|Annabelle Hooper ...|       Paul Serafini|Bailee Madison, R...|       United States| August 1, 2017|2016|   TV-PG|  92 min|Children & Family...|While vacationing...|\n",
            "|            80185803|TV Show| Aussie Gold Hunters|                null|                null|           Australia| August 1, 2017|2016|   TV-MA|1 Season|Docuseries, Inter...|Three teams of go...|\n",
            "|            80147321|  Movie|     Below Her Mouth|        April Mullen|Erika Linder, Nat...|              Canada| August 1, 2017|2016|   TV-MA|  91 min|Dramas, Independe...|An engaged fashio...|\n",
            "|            80157177|TV Show| Bountiful Blessings|                null|Jessica Hsuan, Ta...|                null| August 1, 2017|2011|   TV-14|1 Season|International TV ...|A former mortal w...|\n",
            "|            80185369|TV Show|Close Your Eyes B...|                null|Bryan Shu-Hao Cha...|              Taiwan| August 1, 2017|2016|   TV-14|1 Season|Crime TV Shows, I...|While vacationing...|\n",
            "|            70248183|  Movie|         Cloud Atlas|Lilly Wachowski, ...|Tom Hanks, Halle ...|Germany, United S...| August 1, 2017|2012|       R| 172 min|Action & Adventur...|In this star-stud...|\n",
            "|            80185723|  Movie|        Cop Watchers|          Ben Steele|                null|                null| August 1, 2017|2016|   TV-MA|  45 min|Documentaries, In...|Explore the stori...|\n",
            "|            80159897|TV Show|           Entangled|                null|Thomas Ong, Jack ...|         South Korea| August 1, 2017|2014|   TV-MA|1 Season|International TV ...|When the children...|\n",
            "|# The wave for is...|   null|                null|                null|                null|                null|           null|null|    null|    null|                null|                null|\n",
            "|            80166472|TV Show|              Fartsa|                null|Alexander Petrov,...|              Russia| August 1, 2017|2015|   TV-MA|1 Season|Crime TV Shows, I...|As four Russian f...|\n",
            "|            80185801|TV Show|Genius of the Anc...|                null|      Bettany Hughes|      United Kingdom| August 1, 2017|2015|   TV-PG|1 Season|British TV Shows,...|Historian Bettany...|\n",
            "|            80186252|TV Show|Genius of the Mod...|                null|      Bettany Hughes|      United Kingdom| August 1, 2017|2016|   TV-14|1 Season|British TV Shows,...|Historian Bettany...|\n",
            "|            80151962|  Movie|      Handsome Devil|         John Butler|Fionn O'Shea, Nic...|             Ireland| August 1, 2017|2016|   TV-MA|  95 min|Comedies, Dramas,...|A self-described ...|\n",
            "|            80185427|TV Show|Have You Ever Fal...|                null|Lan Cheng-lung, E...|                null| August 1, 2017|2016|   TV-MA|1 Season|Crime TV Shows, I...|A new teacher fin...|\n",
            "|            80176179|TV Show|Hogie the Globeho...|                null|        Mae Elliessa|       United States| August 1, 2017|2016|    TV-Y|1 Season|            Kids' TV|Hogie and his fri...|\n",
            "|            80190146|  Movie|Jessica Darling's...|           Ali Scher|Chloe East, Emma ...|       United States| August 1, 2017|2016|    TV-G|  80 min|Children & Family...|Armed with popula...|\n",
            "|            80184595|TV Show|   Life Plan A and B|                null|Rainie Yang, Yan ...|              Taiwan| August 1, 2017|2016|   TV-PG|1 Season|International TV ...|Two parallel life...|\n",
            "|            80166481|TV Show|              Locust|                null|Pyotr Fyodorov, P...|              Russia| August 1, 2017|2014|   TV-MA|1 Season|Crime TV Shows, I...|A torrid affair b...|\n",
            "|            80184684|TV Show|          Love Storm|                null|Chris Wu, Chen Yu...|                null| August 1, 2017|2016|   TV-PG|1 Season|International TV ...|Three grown sibli...|\n",
            "|            80179433|  Movie|Maz Jobrani: Immi...|         Maz Jobrani|         Maz Jobrani|                null| August 1, 2017|2017|   TV-MA|  67 min|     Stand-Up Comedy|Iranian American ...|\n",
            "|            80185622|TV Show|     Nurses Who Kill|                null|                null|      United Kingdom| August 1, 2017|2016|   TV-14|1 Season|British TV Shows,...|Top medical, crim...|\n",
            "+--------------------+-------+--------------------+--------------------+--------------------+--------------------+---------------+----+--------+--------+--------------------+--------------------+\n",
            "only showing top 150 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGzPDicW4zUo"
      },
      "source": [
        "\n",
        "**Problem 1**: First column contains a mix of numeric and string data.\n",
        "\n",
        "\n",
        "We can also use the `.printSchema()` method to print the inferred schema associated with the data. Notice that we have 12 columns (which is expected based on our format information) but there are no column names, incorrect datatypes, and each field is nullable. *Note: `.printSchema()` is also a Spark action.*\n",
        "\n",
        "**Problem 2**: Column names are generic.\n",
        "\n",
        "**Problem 3**: All columns are typed as strings, but appear to contain various datatypes (also reference **problem 1**)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THf1tctucd9h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dd6a3c4-ae41-4e60-d8ea-e91040d86c03"
      },
      "source": [
        "# Print schema\n",
        "df.printSchema()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- _c1: string (nullable = true)\n",
            " |-- _c2: string (nullable = true)\n",
            " |-- _c3: string (nullable = true)\n",
            " |-- _c4: string (nullable = true)\n",
            " |-- _c5: string (nullable = true)\n",
            " |-- _c6: string (nullable = true)\n",
            " |-- _c7: string (nullable = true)\n",
            " |-- _c8: string (nullable = true)\n",
            " |-- _c9: string (nullable = true)\n",
            " |-- _c10: string (nullable = true)\n",
            " |-- _c11: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXGTHdV75wUi"
      },
      "source": [
        "## **Bypassing the CSV interpeter**\n",
        "\n",
        "Our first few data rows look ok, but we can see that we have a few random rows even in our small sample. We know the first column should be an integer value but it looks like there are some values that do not meet this requirement. \n",
        "\n",
        "Let's run a `.count()` method on our dataframe to determine how many rows are present in the dataset, regardless of whether they're correct.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eABNrdsfOzRR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d4dcb5a-dea0-455c-e399-f25d64247887"
      },
      "source": [
        "# Count the number of rows\n",
        "df.count()\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6238"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYuq7DGUPtSe"
      },
      "source": [
        " Let's run a quick select statement on the DataFrame to determine the makeup of the content. We'll use some methods from the `pyspark.sql.functions` module to help us determine this. We'll alias this module as `F` for simplicity.\n",
        "\n",
        "We're going to use the `F.col` method to find the column named `_c0` in our dataframe. We'll then chain the function `.cast(\"int\")` to attempt to change each entry from a string value to an integer one. We then use the `.isNotNull()` function to find only entries that are not null. This is passed to the the `.filter()` method on the dataframe to return only rows that meet this requirement. Finally we run the `.count()` method on the resulting dataframe to get the count of rows where the first column is an integer value.\n",
        "\n",
        "An example to filter a Spark dataframe named `df_1` with a column named `_c0` would be:\n",
        "\n",
        "```\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "df_1.filter(F.col(\"_c0\").cast(\"int\").isNotNull()).show()\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uznFuRdDcwTM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d6ce2470-28ff-476e-bd6f-5d3ff330b8fd"
      },
      "source": [
        "# Import the Pyspark SQL helper functions\n",
        "from pyspark.sql import functions as F\n",
        "\n",
        "# Determine how many rows have a column that converts properly to an integer value\n",
        "df.filter(F.col(\"_c0\").cast(\"int\").isNotNull()).count()\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6173"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "6238-6173"
      ],
      "metadata": {
        "id": "r8aESNVQ_vFM",
        "outputId": "71eceddf-dace-4879-a93d-385fa5723037",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "65"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7_8YXWoS59q"
      },
      "source": [
        "### **Spark is different**\n",
        "\n",
        "Depending on your background, you may be very confused as to what's going on right now. If you're used to pandas, you should know that Spark behaves similarly, but there are some significant differences.\n",
        "\n",
        "#### **Spark dataframes are immutable**\n",
        "- This means that once a dataframe is created, you cannot change the contents of the dataframe, you can only create a new one.\n",
        "- We won't cover why during this class, beyond mentioning that it makes a distributed system (such as Spark) much more manageable.\n",
        "- While you cannot change a dataframe, you can create a new one.\n",
        "- Dataframes in Spark are defined with various types of *transformations*. These are the *lazy* commands we mentioned earlier. You can think of them as a recipe, or set of commands that will be run at a given time. \n",
        "- Spark *actions* (such as `.count()`, `.show()`, `.write()`, among others) instantiate the dataframe, meaning that the data is processed and available within the dataframe.\n",
        "\n",
        "The important thing to note is that these `.filter()` commands are not changing our underlying dataframe - it's creating a new dataframe with the results of the `.filter()` operation and giving us the `.count()` of that. This is all done behind the scenes for you, but it's important to understand what Spark is doing underneath.\n",
        "\n",
        "### **Back to analysis**\n",
        "\n",
        "Now that we've determined there is a difference between the number of correct entries vs all entries (ie, take the full count and subtract the filtered dataframe), let's look at the rows that aren't coverting properly.\n",
        "\n",
        "We'll do the same as before, but this time we're going to use the `.isNull()` method to obtain only the rows that can't be cast to integers. We'll use the `.show()` method to see the content.\n",
        "\n",
        "*Note, this is only possible because Spark maintains an immutable dataframe. In our previous step, we created a new dataframe from our `.filter()` command, ran an action, and then Spark threw away the dataframe as we didn't assign it to a variable. We're going to now do the same type of operation on the original `titles_df` dataframe.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FM8wY72J7qnC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d95bd71-06a5-4495-f389-cfca51396a26"
      },
      "source": [
        "# Look at rows that don't convert properly\n",
        "df.filter(F.col(\"_c0\").cast(\"int\").isNull()).show()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+--------------------+--------------------+----+-----+----+----+----+----+----+----+----+\n",
            "|                 _c0|                 _c1|                 _c2| _c3|  _c4| _c5| _c6| _c7| _c8| _c9|_c10|_c11|\n",
            "+--------------------+--------------------+--------------------+----+-----+----+----+----+----+----+----+----+\n",
            "|# The wave for is...|                null|                null|null| null|null|null|null|null|null|null|null|\n",
            "|             TV Show|               TV-MA|Crime TV Shows, I...|null| null|null|null|null|null|null|null|null|\n",
            "|# Which was, Chan...|                null|                null|null| null|null|null|null|null|null|null|null|\n",
            "|# Five 40 dangero...|                null|                null|null| null|null|null|null|null|null|null|null|\n",
            "|# Heisei Hyakkei ...|                null|                null|null| null|null|null|null|null|null|null|null|\n",
            "|# (or will advanc...|                null|                null|null| null|null|null|null|null|null|null|null|\n",
            "|         The Miracle|Donghyun Kim, Nah...|International TV ...|null| null|null|null|null|null|null|null|null|\n",
            "|               Movie|Harold and Lillia...|Harold Michelson,...|2015|TV-14|null|null|null|null|null|null|null|\n",
            "|# Site Montana en...|                null|                null|null| null|null|null|null|null|null|null|null|\n",
            "|    Black Snake Moan|                null|                null|null| null|null|null|null|null|null|null|null|\n",
            "|# To improving ex...|                null|                null|null| null|null|null|null|null|null|null|null|\n",
            "|# Stanford Linear...|                null|                null|null| null|null|null|null|null|null|null|null|\n",
            "|# Europeans. Carl...|                null|                null|null| null|null|null|null|null|null|null|null|\n",
            "|# Offer internati...|                null|                null|null| null|null|null|null|null|null|null|null|\n",
            "|         Jen Kirkman|        May 22, 2015|                null|null| null|null|null|null|null|null|null|null|\n",
            "|# Hundred Views S...|                null|                null|null| null|null|null|null|null|null|null|null|\n",
            "|# Filipino Americ...|                null|                null|null| null|null|null|null|null|null|null|null|\n",
            "|# West Side desce...|                null|                null|null| null|null|null|null|null|null|null|null|\n",
            "|# To British colo...|                null|                null|null| null|null|null|null|null|null|null|null|\n",
            "|# Rates than laws...|                null|                null|null| null|null|null|null|null|null|null|null|\n",
            "+--------------------+--------------------+--------------------+----+-----+----+----+----+----+----+----+----+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jw0aLgc19Qok"
      },
      "source": [
        "*Data problems*:\n",
        "- **Problem 4**: Comment rows - These begin with a `#` character in the first column, and all other columns are null\n",
        "- **Problem 5**: Missing first column - We have few rows that reference `TV Show` or `Movie`, which should be the 2nd column.\n",
        "- **Problem 6**: Odd columns - There are a few rows included where the columns seem out of sync (ie, a content type in the ID field, dates in the wrong column, etc).\n",
        "\n",
        "We could fairly easily remove rows that match this pattern, but we're not entirely sure what to expect here. This is a common issue when trying to parse a large amount of data, be it in native Python, in Spark, or even with command-line tools. \n",
        "\n",
        "What we need to do is bypass most of the CSV parser's intelligence, but still load the content into a DataFrame. One way to do this is to modify an option on the CSV loader.\n",
        "\n",
        "# **CSV loading**\n",
        "\n",
        "Our initial import relies on the defaults for the CSV import mechanism. This typically assumes an actual comma-separated value file using `,` between fields and a normal row level terminator (ie, `\\r\\n`, `\\r`, `\\n`). While this often works well, it doesn't always handle ever data cleaning process you'd like, especially if you want to save the errant data for later examination.\n",
        "\n",
        "One way we can trick our CSV load is to specify a custom separator that we know does not exist within our dataset. As we used above, the option to do this is called `sep` and takes a single character to be used as the column separator. The separator cannot be an empty string so depending on your data, you may need to determine a character that is not used. For our purposes, let's use a curly brace, `{`, which is most likely not present in our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vme6VYgc8UP7"
      },
      "source": [
        "# Load the files into a DataFrame with a single column\n",
        "df = spark.read.csv(\"/tmp/netflix_titles_dirty*.csv.gz\",header=False, sep=\"{\")\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyTxTDhXART2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e90bb492-7840-48f3-f5c4-97c771057b44"
      },
      "source": [
        "# Count rows\n",
        "df.count()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6238"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JcsIM-2KBMzP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1df30fc-7c44-4d49-f978-9b10e11e2a97"
      },
      "source": [
        "# Show header\n",
        "df.show(10)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                 _c0|\n",
            "+--------------------+\n",
            "|81002212\\tMovie\\t...|\n",
            "|80176707\\tMovie\\t...|\n",
            "|80199381\\tMovie\\t...|\n",
            "|80202176\\tMovie\\t...|\n",
            "|81014293\\tTV Show...|\n",
            "|81014811\\tTV Show...|\n",
            "|70123118\\tMovie\\t...|\n",
            "|80175913\\tTV Show...|\n",
            "|80182274\\tMovie\\t...|\n",
            "|80185166\\tMovie\\t...|\n",
            "+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0HJfU--BRm1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34a04fc4-5b90-440a-c38f-9136cc05750c"
      },
      "source": [
        "# Print schema\n",
        "df.printSchema()\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uyelt4SeaiHe"
      },
      "source": [
        "# **Q&A?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hSTcSJgEB-YK"
      },
      "source": [
        "# **Cleaning up our data**\n",
        "\n",
        "We know from some earlier analysis that we have comment rows in place (ie, rows that begin with a `#`). While Spark provides a DataFrame option to handle this automatically, let's consider what it would take to remove comment rows.\n",
        "\n",
        "We need to:\n",
        "\n",
        "- Determine if the column / line starts with a `#`\n",
        "- If so, filter these out to a new DataFrame\n",
        "\n",
        "There are many ways to accomplish this in Spark, but let's use a conceptually straight-forward option, `.startsWith()`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pny8lhMnBVsv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3d0a3af-5c7e-4673-e6c1-c14c5473cbb0"
      },
      "source": [
        "# Filter DataFrame and show rows that starts with #\n",
        "df.select('_c0').show()\n"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+\n",
            "|                 _c0|\n",
            "+--------------------+\n",
            "|81002212\\tMovie\\t...|\n",
            "|80176707\\tMovie\\t...|\n",
            "|80199381\\tMovie\\t...|\n",
            "|80202176\\tMovie\\t...|\n",
            "|81014293\\tTV Show...|\n",
            "|81014811\\tTV Show...|\n",
            "|70123118\\tMovie\\t...|\n",
            "|80175913\\tTV Show...|\n",
            "|80182274\\tMovie\\t...|\n",
            "|80185166\\tMovie\\t...|\n",
            "|80068219\\tMovie\\t...|\n",
            "|70209163\\tMovie\\t...|\n",
            "|80170687\\tTV Show...|\n",
            "|80163191\\tMovie\\t...|\n",
            "|80184661\\tMovie\\t...|\n",
            "|80114588\\tTV Show...|\n",
            "|80078761\\tMovie\\t...|\n",
            "|80115844\\tMovie\\t...|\n",
            "|80114111\\tMovie\\t...|\n",
            "|80108998\\tMovie\\t...|\n",
            "+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GOHWXvmDhtu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb827f42-fe3a-4274-e01f-408faa272391"
      },
      "source": [
        "# Count number of rows\n",
        "\n",
        "df.filter(~F.col('_c0').startswith('#')).count()"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6191"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MurSx_92YsGv"
      },
      "source": [
        "We've determined that we have 47 rows that begin with a comment character. We can now easily filter these from our DataFrame as we do below. This resolves **Problem 4**.\n",
        "\n",
        "*Note: We're doing things in a more difficult fashion than is absolutely necessary to illustrate options. The Spark CSV reader has an option for a `comment` property, which actually defaults to skipping all rows starting with a `#` character. That said, it only supports a single character - consider if you were looking for multi-character options (ie, a // from C-style syntax). This feature is also only available in newer versions of Spark, where our method works in any of the 2.x Spark releases.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnKLL42OXtkx"
      },
      "source": [
        "# Filter out comments\n",
        "\n",
        "df = df.filter(~F.col('_c0').startswith('#'))"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHbyFQt4aUag",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a86e634a-4c6b-488e-b059-0bb6bddae108"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6191"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2NRGmdeqa2L3"
      },
      "source": [
        "# **Checking column counts**\n",
        "\n",
        "Our next step for cleaning this dataset in Pyspark involves determining how many columns we should have in our dataset and take care of any outliers. We know from our earlier examination that the dataset should have 12 usable columns per row.\n",
        "\n",
        "First, let's determine how many columns are present in the data and add that as a column. We'll do this with a combination of:\n",
        "\n",
        "- `F.split()`: This acts similar to the Python `split()` method, splitting the contents of a dataframe column on a specified character into a Spark ArrayType column (ie, Spark's version of a list variable). \n",
        "- `F.size()`: Returns the size (length) of a Spark ArrayType column.\n",
        "- `.withColumn()`: Creates a new dataframe with a given column."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0mNCitULaWL7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02b04589-e4af-4898-df1f-49bba48e0990"
      },
      "source": [
        "# Add a column representing the total number of fields / columns\n",
        "df = df.withColumn(\"num_columns\", F.size(F.split('_c0', '\\t')))\n",
        "df.show()"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+\n",
            "|                 _c0|num_columns|\n",
            "+--------------------+-----------+\n",
            "|81002212\\tMovie\\t...|         12|\n",
            "|80176707\\tMovie\\t...|         12|\n",
            "|80199381\\tMovie\\t...|         12|\n",
            "|80202176\\tMovie\\t...|         12|\n",
            "|81014293\\tTV Show...|         12|\n",
            "|81014811\\tTV Show...|         12|\n",
            "|70123118\\tMovie\\t...|         12|\n",
            "|80175913\\tTV Show...|         12|\n",
            "|80182274\\tMovie\\t...|         12|\n",
            "|80185166\\tMovie\\t...|         12|\n",
            "|80068219\\tMovie\\t...|         12|\n",
            "|70209163\\tMovie\\t...|         12|\n",
            "|80170687\\tTV Show...|         12|\n",
            "|80163191\\tMovie\\t...|         12|\n",
            "|80184661\\tMovie\\t...|         12|\n",
            "|80114588\\tTV Show...|         12|\n",
            "|80078761\\tMovie\\t...|         12|\n",
            "|80115844\\tMovie\\t...|         12|\n",
            "|80114111\\tMovie\\t...|         12|\n",
            "|80108998\\tMovie\\t...|         12|\n",
            "+--------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lv5MOxL_tuyw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbf74626-e5c2-488d-cae6-6148dfd2e2bb"
      },
      "source": [
        "# Show rows with a fieldcount > 12 (Note, select statement here isn't necessarily required - used to reorder the columns for easier viewing)\n",
        "df.filter(F.col('num_columns') > 12).show()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+\n",
            "|                 _c0|num_columns|\n",
            "+--------------------+-----------+\n",
            "|80167646\\tMovie\\t...|         32|\n",
            "|80118918\\tMovie\\t...|         30|\n",
            "|80104198\\tTV Show...|         34|\n",
            "|80207445\\tMovie\\t...|         35|\n",
            "|60000722\\tMovie\\t...|         35|\n",
            "|80159714\\tTV Show...|         21|\n",
            "|80134695\\tTV Show...|         30|\n",
            "|81033086\\tMovie\\t...|         32|\n",
            "|70044693\\tMovie\\t...|         33|\n",
            "|80004534\\tMovie\\t...|         31|\n",
            "|80218819\\tTV Show...|         33|\n",
            "|70185023\\tMovie\\t...|         23|\n",
            "|80091778\\tTV Show...|         28|\n",
            "|81121186\\tMovie\\t...|         17|\n",
            "|80099016\\tMovie\\t...|         20|\n",
            "|70113006\\tMovie\\t...|         26|\n",
            "|60004476\\tMovie\\t...|         23|\n",
            "|70286029\\tMovie\\t...|         27|\n",
            "|70039645\\tMovie\\t...|         35|\n",
            "|80198794\\tTV Show...|         15|\n",
            "+--------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pl3Tz5rht5OC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "468bb539-7962-436a-95fd-c1059b74dbd7"
      },
      "source": [
        "# Check for any rows with fewer than 12 columns\n",
        "df.filter(F.col('num_columns') < 12).show()\n"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+\n",
            "|                 _c0|num_columns|\n",
            "+--------------------+-----------+\n",
            "|TV Show\\tTV-MA\\tC...|          3|\n",
            "|80109038\\tTV Show...|         11|\n",
            "|80227114\\tMovie\\t...|         11|\n",
            "|80108518\\tAnthony...|          8|\n",
            "|The Miracle\\tDong...|          3|\n",
            "|80179762\\tShe-Ra ...|         11|\n",
            "|Movie\\tHarold and...|          5|\n",
            "|    Black Snake Moan|          1|\n",
            "|70178618\\tShark N...|         10|\n",
            "|80160504\\tLa Doña...|         10|\n",
            "|80195049\\tMovie\\t...|          5|\n",
            "|Jen Kirkman\\tMay ...|          2|\n",
            "|81077597\\tMovie\\t...|         10|\n",
            "|Movie\\t\\tThis fun...|          3|\n",
            "|81047677\\tMovie\\t...|         10|\n",
            "|\\tKaycie Chase, D...|          9|\n",
            "|80223136\\tMovie\\t...|         10|\n",
            "|80020131\\tKate Hi...|          8|\n",
            "|            70213078|          1|\n",
            "|The 2000s\\t\\tUnit...|          5|\n",
            "+--------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S71Avac6k1h3"
      },
      "source": [
        "**Problem 7**: Column counts do not match our expected schema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmqGCyxi7peV"
      },
      "source": [
        "# Save these to a separate dataframe for later analysis\n",
        "\n",
        "df = df.filter(F.col('num_columns') == 12)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.show()"
      ],
      "metadata": {
        "id": "BOvFufutQiIi",
        "outputId": "a24cc278-792a-4a05-8a5c-4e51ef8d47d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+\n",
            "|                 _c0|num_columns|\n",
            "+--------------------+-----------+\n",
            "|81002212\\tMovie\\t...|         12|\n",
            "|80176707\\tMovie\\t...|         12|\n",
            "|80199381\\tMovie\\t...|         12|\n",
            "|80202176\\tMovie\\t...|         12|\n",
            "|81014293\\tTV Show...|         12|\n",
            "|81014811\\tTV Show...|         12|\n",
            "|70123118\\tMovie\\t...|         12|\n",
            "|80175913\\tTV Show...|         12|\n",
            "|80182274\\tMovie\\t...|         12|\n",
            "|80185166\\tMovie\\t...|         12|\n",
            "|80068219\\tMovie\\t...|         12|\n",
            "|70209163\\tMovie\\t...|         12|\n",
            "|80170687\\tTV Show...|         12|\n",
            "|80163191\\tMovie\\t...|         12|\n",
            "|80184661\\tMovie\\t...|         12|\n",
            "|80114588\\tTV Show...|         12|\n",
            "|80078761\\tMovie\\t...|         12|\n",
            "|80115844\\tMovie\\t...|         12|\n",
            "|80114111\\tMovie\\t...|         12|\n",
            "|80108998\\tMovie\\t...|         12|\n",
            "+--------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ag8tZcQ68gzr"
      },
      "source": [
        "# Determine total number of \"bad\" rows\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCDT2lN4laxn"
      },
      "source": [
        "We can now resolve **problems 5, 6, and 7** by accessing only the rows that have 12 columns present."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR_BbSWA8ila"
      },
      "source": [
        "# Set the dataframe without the bad rows\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkyXBTyv8phE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8e61bfb-4275-4c66-c39e-81ae64265bd8"
      },
      "source": [
        "# How many current rows in dataframe?\n",
        "\n",
        "df.count()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6113"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_QN8__l81xh"
      },
      "source": [
        "# **Q&A**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYMSHgfk9ukp"
      },
      "source": [
        "# **More cleaning / prep**\n",
        "\n",
        "Now that we've removed rows that don't fit our basic formatting, let's continue on with making our dataframe more useful.\n",
        "\n",
        "First, let's create a new column that is a list (actually a Spark ArrayType column) containing all \"columns\" using the `pyspark.sql.functions.split` method. We'll call this `splitcolumn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdV-yNUpAkr3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "928ef2ad-8dae-4d85-b00d-d348f855f1d4"
      },
      "source": [
        "# Create a list of split strings as a new column named splitcolumn\n",
        "df.show()"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------------+-----------+\n",
            "|                 _c0|num_columns|\n",
            "+--------------------+-----------+\n",
            "|81002212\\tMovie\\t...|         12|\n",
            "|80176707\\tMovie\\t...|         12|\n",
            "|80199381\\tMovie\\t...|         12|\n",
            "|80202176\\tMovie\\t...|         12|\n",
            "|81014293\\tTV Show...|         12|\n",
            "|81014811\\tTV Show...|         12|\n",
            "|70123118\\tMovie\\t...|         12|\n",
            "|80175913\\tTV Show...|         12|\n",
            "|80182274\\tMovie\\t...|         12|\n",
            "|80185166\\tMovie\\t...|         12|\n",
            "|80068219\\tMovie\\t...|         12|\n",
            "|70209163\\tMovie\\t...|         12|\n",
            "|80170687\\tTV Show...|         12|\n",
            "|80163191\\tMovie\\t...|         12|\n",
            "|80184661\\tMovie\\t...|         12|\n",
            "|80114588\\tTV Show...|         12|\n",
            "|80078761\\tMovie\\t...|         12|\n",
            "|80115844\\tMovie\\t...|         12|\n",
            "|80114111\\tMovie\\t...|         12|\n",
            "|80108998\\tMovie\\t...|         12|\n",
            "+--------------------+-----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ku9EWddB8Su"
      },
      "source": [
        "# View the contents\n",
        "df = df.withColumn(\"splitColumn\", F.split('_c0', '\\t'))"
      ],
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwUcLor4IggD"
      },
      "source": [
        "# **Creating typed columns**\n",
        "\n",
        "There are several ways to do this operation depending on your needs, but for this dataset we'll explicitly convert the strings in the ArrayType column (ie, a Spark list column) to actual dataframe columns. The `.getItem()` method returns the value at the specified index of the listcolumn (ie, of `splitcolumn`). \n",
        "\n",
        "Let's consider if we wanted to create a full dataframe from the following example dataframe (`df_1`):\n",
        "\n",
        "splitcolumn |\n",
        "---|\n",
        "[1,USA,North America]\n",
        "[2,France,Europe]\n",
        "[3,China,Asia]\n",
        "\n",
        "```\n",
        "df_1 = df_1.withColumn('country_id', df_1.splitcolumn.getItem(0).cast(IntegerType())\n",
        "df_1 = df_1.withColumn('country_name', df_1.splitcolumn.getItem(1))\n",
        "df_1 = df_1.withColumn('continent', df_1.splitcolumn.getItem(2))\n",
        "```\n",
        "\n",
        "This would give you a resulting dataframe of:\n",
        "\n",
        "splitcolumn | country_id | country_name | continent\n",
        "---|---|---|---\n",
        "[1,USA,North America]|1|USA|North America\n",
        "[2,France,Europe]|2|France|Europe\n",
        "[3,China,Asia]|3|China|Asia\n",
        "\n",
        "The `splitcolumn` is currently still present - we'll take care of that later on.\n",
        "\n",
        "Note that for `show_id` and `release_year`, we'll also use `.cast()` to specify them as integers rather than just strings. We also need to import the `IntegerType` from the `pyspark.sql.types` module to properly cast our data to an integer column in Spark.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Roo0kxtlB-hz"
      },
      "source": [
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "# Create columns with specific data types using .getItem()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LytioBfiJLvr"
      },
      "source": [
        "Let's now drop our columns that aren't needed anymore. These are `_c0` (the original single line string), `fieldcount`, and `splitcolumn`. You can drop these as a single column per entry, or a comma-separated set of column names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feZwxQ8rC1MJ"
      },
      "source": [
        "# Drop original _c0 column\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7G_Zdg7EJq-i"
      },
      "source": [
        "Let's verify our content, check the row count, and then look at our schema."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eL_ewudgDKKt"
      },
      "source": [
        "# Showcase new DataFrame\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-C2jZOZEnnr"
      },
      "source": [
        "# Count rows\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fti0um-RDRVp"
      },
      "source": [
        "# Print the schema\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d2W5ImNkJ3f8"
      },
      "source": [
        "# **Even more cleanup**\n",
        "\n",
        "With our last set of steps, we've successfully fixed our remaining **problems (1, 2, and 3)**. Now that we have a generally clean dataset, let's look for further issues.\n",
        "\n",
        "If we look at the distinct values available in the show `type` column, we see an issue. This is a categorical data column - collapsing the values in this column is a typical data cleaning issue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YP9IzxLPFwBS"
      },
      "source": [
        "# Check out unique items in type column\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nx7weVd1G33o"
      },
      "source": [
        "# Isolate rows where type is \"\"\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TW4sUhBMKt9v"
      },
      "source": [
        "You'll notice that we have 5 rows where the type is not specified when one should be. \n",
        "\n",
        "**Problem 8**: Invalid entry in a column - The type column should contain only `TV Show` or `Movie`, but it also contains an empty string value.\n",
        "\n",
        "\n",
        " We have a couple of options:\n",
        "\n",
        "- Drop the rows\n",
        "- Infer what the `type` is based on other content available in the dataset\n",
        "\n",
        "You could remove the problem rows using something like:\n",
        "\n",
        "```\n",
        "titles_cleaned_df = titles_cleaned_df.where('type == \"TV Show\" or type == \"Movie\"')\n",
        "```\n",
        "\n",
        "That feels a bit like cheating though - let's consider how else we could determine this largely automatically.\n",
        "\n",
        "If you look at the `duration` column, you'll notice that there are different meanings behind the entries. We have durations that contain the word *min* (minutes) or the word `Season` for seasons of the show. We can try to use these to properly set the `type` value for these rows.\n",
        "\n",
        "This problem is a bit tricky though as Spark does not have the concept of updating data within a column without jumping through several hoops. We can however work through this issue using a User Defined Function, or UDF.\n",
        "\n",
        "## **UDF**\n",
        "\n",
        "If you haven't worked with them before, a UDF is a Python function that gets applied to every row of a DataFrame. They are extremely flexible and can help us work through issues such as our current one.\n",
        "\n",
        "A UDF in Pyspark requires three things:\n",
        "\n",
        "1. **A Python function or callable**: This is the function that you want called when the UDF is run by Spark.\n",
        "2. **A UDF variable**: Defined by the `udf()` function, with the Python callable defined in #1, and the Spark return type.\n",
        "3. **A Spark transformation**: The UDF must be defined via a transformation (ie `.withColumn()`) to be applied to the dataframe.\n",
        "\n",
        "Consider the following dataframe `df_1`, containing a two fields, `a` and `b`\n",
        "\n",
        "a|b\n",
        "---|---\n",
        "1|2\n",
        "2|3\n",
        "3|4\n",
        "\n",
        "For illustration, let's say we want to use a UDF to define a new column, which is simply the value `a*b`, unless the value of `a` is `3`. If it is, then we want the value to be `0`.\n",
        "\n",
        "Let's define our function - taking two arguments, `a`, and `b`. \n",
        "\n",
        "```\n",
        "def multiply(a, b):\n",
        "  if a == 3:\n",
        "    return 0\n",
        "  else:\n",
        "    return a*b\n",
        "```\n",
        "\n",
        "Now, we need to define our UDF variable. We need to import `udf` from `pyspark.sql.functions`, and as we're returning an integer value, we need to import `IntegerType` from `pyspark.sql.types`. \n",
        "\n",
        "```\n",
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import IntegerType\n",
        "\n",
        "udfMultiply = udf(multiply, IntegerType())\n",
        "```\n",
        "\n",
        "Note that the `udf` function only takes the name of the callable, not any arguments. Those are defined with our last step - defining a new column. \n",
        "\n",
        "```\n",
        "df_1 = df_1.withColumn('output', udfMultiply(F.col('a'), F.col('b')))\n",
        "```\n",
        "\n",
        "Note that in this instance we're using the `F.col` method to refer to the column. You could use any of the other valid Spark methods to specify a dataframe column (ie, `df_1.a`, `df_1['b']`, etc).\n",
        "\n",
        "Assuming we run a Spark action, such as `.show()`, we would get a dataframe with the following contents:\n",
        "\n",
        "a|b|output\n",
        "---|---|---\n",
        "1|2|2\n",
        "2|3|6\n",
        "3|4|0\n",
        "\n",
        "*Note this is a trivial example for illustration purposes - the same behavior would be much better implemented using assorted functions in the Spark libraries.*\n",
        "\n",
        "\n",
        "First, let's define our Python function that takes two arguments, a showtype (ie, *Movie*, *TV Show*, or other) and the showduration. Note that these are strings in this case. We'll check if the showtype is already a Movie or TV Show - if so, just return that value. Otherwise, we'll check if the showduration ends with *min*, indicating a Movie. If not, we'll specify it as a TV Show.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m97hs-7qPCus"
      },
      "source": [
        "# Define the UDF callable\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PW5L773xQCfy"
      },
      "source": [
        "# **Define the UDF for Spark**\n",
        "\n",
        "Now we need to configure the UDF for Spark to access it accordingly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y7TpzkAQb8x"
      },
      "source": [
        "from pyspark.sql.functions import udf\n",
        "from pyspark.sql.types import StringType\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sE7GaRVR7Tv"
      },
      "source": [
        "# Create a new derived column, passing in the appropriate values\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-eGdUm5ZSojy"
      },
      "source": [
        "# Show the rows where type is an empty string again, examining the derivedType\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1Z923yGSt2q"
      },
      "source": [
        "# Drop the original type column and rename derviedType to type\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JW7Dtb-wHfs5"
      },
      "source": [
        "# Verify we only have two types available\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXr5D8-4HkSR"
      },
      "source": [
        "# Verify our row count is the same\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ks9eP6uMnUPe"
      },
      "source": [
        "We've now successfully resolved **problem 8**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KUVyBzLTZgN"
      },
      "source": [
        "# **Saving data for analysis / further processing**\n",
        "\n",
        "The last step of our data cleaning is to save the cleaned dataframe out to a file type. If you plan to do any further analysis or processing using Spark, it's highly recommended you use Parquet. Other options are available per your needs, but Spark is optimized to take advantage of Parquet.\n",
        "\n",
        "There are two options we use for the `.write.parquet()` method:\n",
        "\n",
        "- The path of where to write the file\n",
        "- An optional `mode` parameter, which we've set to `overwrite`. This allows Spark to write data to an existing location, solving some potential issues in a notebook environment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "inKlmeIfHnw9"
      },
      "source": [
        "# Save the data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ay7w3C4iqRXp"
      },
      "source": [
        "Let's now take a look at the contents using the `ls` shell command. You'll notice that the `/tmp/netflix_titles_cleaned.parquet` location is actually a directory, not just a file. This is due to the way Spark handles its data allocation and formatting. More on this in a minute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5JnlKUzssau"
      },
      "source": [
        "# Is file in directory?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KydUNbnYn-BQ"
      },
      "source": [
        "Note that typically when processing data in Spark, you'll want to use the Parquet format as described above. This is great for any further processing or analysis you plan to do in Spark. However, it can be difficult to read Parquet files outside of Spark without extra work. As such, let's create a version in CSV format that you can download if you desire.\n",
        "\n",
        "We'll need to do four steps for this operation:\n",
        "\n",
        "- Combine the data into a single file using the `.coalesce(1)` transformation. Spark normally keeps data in separate files to improve performance and bypass RAM issues. Our dataset is small and we can bypass those concerns.\n",
        "- Use the `.write.csv()` method (instead of `.write.parquet()`). We'll also add an extra option of `sep='\\t'` to bypass the issue of commas being present in our data. We also have to define a `header=True` component so our columns are named correctly.\n",
        "- Rename the file to something usable with a shell command `mv`. Spark stores files named via their partition id. We need to rename that to something more recognizable.\n",
        "- Finally, we'll use a special command specific to the notebook environment to download the file.\n",
        "\n",
        "As you've seen within Spark, you can chain commands together. As such, we'll combine the first two components."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Km2dxHXGp85h"
      },
      "source": [
        "# Coalesce and save the data in CSV format\n",
        "\n",
        "titles_cleaned_df.coalesce(1).write.csv('/tmp/netflix_titles_cleaned.csv', mode='overwrite', sep='\\t', header=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AvadihO-qnNF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9f48b025-f373-449f-9558-617b87a0d6cf"
      },
      "source": [
        "# Look at the output of the command using the shell command `ls`\n",
        "\n",
        "!ls /tmp/netflix_titles_cleaned.csv"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "part-00000-5f404d8b-c020-478e-856d-76999184e063-c000.csv  _SUCCESS\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCZMYcOKq2VX"
      },
      "source": [
        "# Rename the data file\n",
        "\n",
        "!mv /tmp/netflix_titles_cleaned.csv/part-00000*.csv /tmp/netflix_titles_cleaned_final.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qMaWUVGIrIWT"
      },
      "source": [
        "# Download the file via notebook tools\n",
        "\n",
        "from google.colab import files\n",
        "files.download('/tmp/netflix_titles_cleaned_final.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMJO9w_xUDAh"
      },
      "source": [
        "# **Challenges**\n",
        "\n",
        "We've looked at several data cleaning operations using Spark. Here are some other challenges to consider within the dataset:\n",
        "\n",
        "1) *Splitting names* - \n",
        "  You may have noticed that the names are combined for the cast and directors into a list. Consider how you would turn that data into a list / array column to easily access more detailed information (which shows have the largest cast, etc?)\n",
        "\n",
        "2) *Splitting names further* - Consider taking any of the name fields and splitting it into first name, last name, etc. Take special consideration about how you would handle initials, names with more than 3 components, etc.\n",
        "\n",
        "3) *Parsing dates* - Look at the `date_added` field and determine if and how you could reliably convert this to an actual datetime field.\n",
        "\n",
        "# **Last Q&A**"
      ]
    }
  ]
}